<!DOCTYPE html>
<html class=" no-hovermq no-backdropfilter backgroundcliptext no-cssmask"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Evolution Strategies as a Scalable Alternative to Reinforcement Learning</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="shortcut icon" href="https://blog.openai.com/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="https://blog.openai.com/evolution-strategies/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="OpenAI Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Evolution Strategies as a Scalable Alternative to Reinforcement Learning">
    <meta property="og:description" content="We've discovered that evolution strategies (ES), an optimization technique that's been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL's inconveniences. In particular, ES is simpler to implement (there is no need for backpropagation)">
    <meta property="og:url" content="https://blog.openai.com/evolution-strategies/">
    <meta property="og:image" content="https://blog.openai.com/content/images/2017/03/2x-2.jpg">
    <meta property="article:published_time" content="2017-03-24T16:17:26.000Z">
    <meta property="article:modified_time" content="2017-11-28T19:47:18.000Z">
    <meta property="article:tag" content="Andrej Karpathy">
    <meta property="article:tag" content="Tim Salimans">
    <meta property="article:tag" content="Jonathan Ho">
    <meta property="article:tag" content="Peter Chen">
    <meta property="article:tag" content="Ilya Sutskever">
    <meta property="article:tag" content="John Schulman">
    <meta property="article:tag" content="Greg Brockman">
    <meta property="article:tag" content="Szymon Sidor">
    
    <meta property="article:publisher" content="https://www.facebook.com/openai.research">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Evolution Strategies as a Scalable Alternative to Reinforcement Learning">
    <meta name="twitter:description" content="We've discovered that evolution strategies (ES), an optimization technique that's been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL's inconveniences. In particular, ES is simpler to implement (there is no need for backpropagation)">
    <meta name="twitter:url" content="https://blog.openai.com/evolution-strategies/">
    <meta name="twitter:image" content="https://blog.openai.com/content/images/2017/03/2x-2.jpg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Andrej Karpathy">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Andrej Karpathy, Tim Salimans, Jonathan Ho, Peter Chen, Ilya Sutskever, John Schulman, Greg Brockman, Szymon Sidor">
    <meta name="twitter:site" content="@openai">
    <meta property="og:image:width" content="1276">
    <meta property="og:image:height" content="1696">
    
    <script type="text/javascript" async="" src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/analytics.js"></script><script type="text/javascript" async="" src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/analytics_002.js"></script><script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "OpenAI Blog",
        "logo": {
            "@type": "ImageObject",
            "url": "https://blog.openai.com/content/images/2017/03/twitter.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Andrej Karpathy",
        "url": "https://blog.openai.com/author/andrej/",
        "sameAs": []
    },
    "headline": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
    "url": "https://blog.openai.com/evolution-strategies/",
    "datePublished": "2017-03-24T16:17:26.000Z",
    "dateModified": "2017-11-28T19:47:18.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://blog.openai.com/content/images/2017/03/2x-2.jpg",
        "width": 1276,
        "height": 1696
    },
    "keywords": "Andrej Karpathy, Tim Salimans, Jonathan Ho, Peter Chen, Ilya Sutskever, John Schulman, Greg Brockman, Szymon Sidor",
    "description": "We&#x27;ve discovered that evolution strategies (ES), an optimization technique that&#x27;s been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL&#x27;s inconveniences. In particular, ES is simpler to implement (there is no need for backpropagation)",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://blog.openai.com/"
    }
}
    </script>

    <script type="text/javascript" src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/ghost-sdk.js"></script>
<script type="text/javascript">
ghost.init({
	clientId: "ghost-frontend",
	clientSecret: "710c928a5473"
});
</script>
    <meta name="generator" content="Ghost 1.20">
    <link rel="alternate" type="application/rss+xml" title="OpenAI Blog" href="https://blog.openai.com/rss/">

  <link rel="icon" href="https://blog.openai.com/assets/images/favicon/favicon.ico">
  <link rel="shortcut icon" href="https://blog.openai.com/assets/images/favicon/favicon.ico">
  <link rel="apple-touch-icon" href="https://blog.openai.com/assets/images/favicon/favicon.png">

  <link rel="stylesheet" type="text/css" href="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/style.css">
  

  
<style>.fluidvids {width: 100%; max-width: 100%; position: relative;}.fluidvids-item {position: absolute; top: 0px; left: 0px; width: 100%; height: 100%;}</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body class="post-template tag-andrej-karpathy tag-tim-salimans tag-jonathan-ho tag-peter-chen tag-ilya-sutskever tag-john-schulman tag-greg-brockman tag-hash-research-release-post tag-szymon-sidor tag-hash-evolution-strategies tag-hash-research-release-no-2"><div id="MathJax_Message" style="display: none;"></div>


  <main class="Main">

    <div class="MainContent">

      <div class="Shared-Navigation">
  <div class="Shared-Navigation-logo">
    <a href="https://openai.com/" title="OpenAI" class="Shared-Navigation-link
              Shared-Navigation-link--logo">
      <img class="logo" src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/nav-logo.svg" alt="OpenAI" width="27" height="27">
    </a>
  </div>

  <div class="Shared-Navigation-linksContainer">


        <div class="Shared-Navigation-link
                    Shared-Navigation-mobileNavToggle" ontouchstart=""> 
             <!-- Note: ontouchstart makes :active work -->
          <div class="Shared-Navigation-mobileNavIconContainer">
            <div class="Shared-Navigation-mobileOpen"></div>
            <div class="Shared-Navigation-mobileClose"></div>
          </div>
        </div>

        <div class="Shared-Navigation-MobileContainer Shared-Navigation-MobileContainer--defaultTransition">

          <div class="container-fluid">
      
        <div class="row
                    center-xs">
          <div class="col-xs-9
                      col-sm-6">

            <ul class="Shared-Navigation-list">
              <li class="Shared-Navigation-listItem-0">
                <a class="Shared-Navigation-link Shared-Navigation-link--listLink" href="https://openai.com/research">Research</a>
              </li>
              <li class="Shared-Navigation-listItem-1">
                  <a class="Shared-Navigation-link Shared-Navigation-link--listLink" href="https://openai.com/systems">Systems</a>
              </li>
            </ul>

          </div>
          <div class="col-xs-9
                      col-sm-6">

            <ul class="Shared-Navigation-list
                       Shared-Navigation-list--rightAligned">
              <li class="Shared-Navigation-listItem-2">
                <a class="Shared-Navigation-link Shared-Navigation-link--listLink" href="https://openai.com/about">About</a>
                </li>
              <li class="Shared-Navigation-listItem-3">
                <a class="Shared-Navigation-link
                          Shared-Navigation-link--listLink
                          Shared-Navigation-link--selected" href="https://blog.openai.com/">Blog</a>
              </li>
            </ul>

          </div>
        </div>

     </div>
    </div>
      

  </div><!-- ./Shared-Navigation-linksContainer -->
</div><!-- ./Shared-Navigation -->
      
      

  <article class="Post post tag-andrej-karpathy tag-tim-salimans tag-jonathan-ho tag-peter-chen tag-ilya-sutskever tag-john-schulman tag-greg-brockman tag-hash-research-release-post tag-szymon-sidor tag-hash-evolution-strategies tag-hash-research-release-no-2" id="evolution-strategies">
    






    <header class="ResearchPostHeader
                   ResearchPostHeader--evolution-strategies">

      <section class="ResearchPostHeader-coverContainer">

        <h1 class="ResearchPostHeader-cover">
          <div class="ResearchPostHeader-cover-inner">
            <div class="Shared-Card-glare"></div>
            <img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/1x-no-mark.jpg" srcset="https://d4mucfpksywv.cloudfront.net/research-covers/evolution-strategies/1x-no-mark.jpg 1x,
            https://d4mucfpksywv.cloudfront.net/research-covers/evolution-strategies/2x-no-mark.jpg 2x" alt="Evolution Strategies as a Scalable Alternative to Reinforcement Learning">
          </div>
        </h1>
      
      </section>

      <section class="ResearchPostHeader-intro
                      PostContent">

        <time class="ResearchPostHeader-date" datetime="2017-03-24">March 24, 2017</time>

        <h1 class="ResearchPostHeader-title">
          Evolution Strategies as a Scalable Alternative to Reinforcement Learning
        </h1>

    <div class="kg-card-markdown"><p>We’ve <a href="https://arxiv.org/abs/1703.03864">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong> techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p>
<p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a> along a few random directions at each step.</p>
<div class="Post-Actions Post-Actions--reverse-icons Post-Actions--list"><a href="https://github.com/openai/evolution-strategies-starter" class="Post-Actions--code">View on GitHub</a><a href="https://arxiv.org/abs/1703.03864" class="Post-Actions--paper">View on arXiv</a><a href="#content" class="Post-Actions--continue">Read more</a></div></div></section></header>
<section class="Post-inner PostContent post-content research-release-post" id="content">
<p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">“AlexNet” paper</a>
 showed how to design, scale and train convolutional neural networks 
(CNNs) to achieve extremely strong results on image recognition tasks, 
at a time when most researchers thought that CNNs were not a promising 
approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602">Deep Q-Learning paper</a>
 showed how to combine Q-Learning with CNNs to successfully solve Atari 
games, reinvigorating RL as a research field with exciting experimental 
(rather than theoretical) results. Likewise, our work demonstrates that 
ES achieves strong performance on RL benchmarks, dispelling the common 
belief that ES methods are impossible to apply to high dimensional 
problems.</p>
<p>ES is easy to implement and scale. Running on a computing cluster of 
80 machines and 1,440 CPU cores, our implementation is able to train a 
3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes 
about 10 hours). Using 720 cores we can also obtain comparable 
performance to A3C on Atari while cutting down the training time from 1 
day to 1 hour.</p>
<p>In what follows, we’ll first briefly describe the conventional RL 
approach, contrast that with our ES approach, discuss the tradeoffs 
between ES and RL, and finally highlight some of our experiments.</p>
<h4 id="reinforcementlearning">Reinforcement Learning</h4>
<p>Let’s briefly look at how RL works. Suppose we are given some 
environment (e.g. a game) that we’d like to train an agent on. To 
describe the behavior of the agent, we define a policy function (the 
brain of the agent), which computes how the agent should act in any 
given situation. In practice, the policy is usually a neural network 
that takes the current state of the game as an input and calculates the 
probability of taking any of the allowed actions. A typical policy 
function might have about 1,000,000 parameters, so our task comes down 
to finding the precise setting of these parameters such that the policy 
plays well (i.e. wins a lot of games).</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/first-graphic-1.png" alt=""></p>
<p><em>Above: In the game of Pong, the policy could take the pixels of 
the screen and compute the probability of moving the player’s paddle (in
 green, on right) Up, Down, or neither.</em></p>
<p>The training process for the policy works as follows. Starting from a
 random initialization, we let the agent interact with the environment 
for a while and collect episodes of interaction (e.g. each episode is 
one game of Pong). We thus obtain a complete recording of what happened:
 what sequence of states we encountered, what actions we took in each 
state, and what the reward was at each step. As an example, below is a 
diagram of three episodes that each took 10 time steps in a hypothetical
 environment. Each rectangle is a state, and rectangles are colored 
green if the reward was positive (e.g. we just got the ball past our 
opponent) and red if the reward was negative (e.g. we missed the ball):</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/second-graphic-1.png" alt=""></p>
<p>This diagram suggests a recipe for how we can improve the policy; 
whatever we happened to do leading up to the green states was good, and 
whatever we happened to do in the states leading up to the red areas was
 bad. We can then use backpropagation to compute a small update on the 
network’s parameters that would make the green actions more likely in 
those states in the future, and the red actions less likely in those 
states in the future. We expect that the updated policy works a bit 
better as a result. We then iterate the process: collect another batch 
of episodes, do another update, etc.</p>
<p><strong>Exploration by injecting noise in the actions.</strong> The 
policies we usually use in RL are stochastic, in that they only compute 
probabilities of taking any action. This way, during the course of 
training, the agent may find itself in a particular state many times, 
and at different times it will take different actions due to the 
sampling. This provides the signal needed for learning; some of those 
actions will lead to good outcomes, and get encouraged, and some of them
 will not work out, and get discouraged. We therefore say that we 
introduce exploration into the learning process by injecting noise into 
the agent’s actions, which we do by sampling from the action 
distribution at each time step. This will be in contrast to ES, which we
 describe next.</p>
<h4 id="evolutionstrategies">Evolution Strategies</h4>
<p><strong>On “Evolution”.</strong> Before we dive into the ES approach,
 it is important to note that despite the word “evolution”, ES has very 
little to do with biological evolution. Early versions of these 
techniques may have been inspired by biological evolution and the 
approach can, on an abstract level, be seen as sampling a population of 
individuals and allowing the successful individuals to dictate the 
distribution of future generations. However, the mathematical details 
are so heavily abstracted away from biological evolution that it is best
 to think of ES as simply a class of black-box stochastic optimization 
techniques.</p>
<p><strong>Black-box optimization.</strong> In ES, we forget entirely 
that there is an agent, an environment, that there are neural networks 
involved, or that interactions take place over time, etc. The whole 
setup is that 1,000,000 numbers (which happen to describe the parameters
 of the policy network) go in, 1 number comes out (the total reward), 
and we want to find the best setting of the 1,000,000 numbers. 
Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code> (the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p>
<p><strong>The ES algorithm.</strong> Intuitively, the optimization is a
 “guess and check” process, where we start with some random parameters 
and then repeatedly 1) tweak the guess a bit randomly, and 2) move our 
guess slightly towards whatever tweaks worked better. Concretely, at 
each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code>
 with gaussian noise. We then evaluate each one of the 100 candidates 
independently by running the corresponding policy network in the 
environment for a while, and add up all the rewards in each case. The 
updated parameter vector then becomes the weighted sum of the 100 
vectors, where each weight is proportional to the total reward (i.e. we 
want the more successful candidates to have a higher weight). 
Mathematically, you’ll notice that this is also equivalent to estimating
 the gradient of the expected reward in the parameter space using finite
 differences, except we only do it along 100 random directions. Yet 
another way to see it is that we’re still doing RL (Policy Gradients, or
 <a href="http://www-anw.cs.umass.edu/%7Ebarto/courses/cs687/williams92simple.pdf">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/evo.png" alt=""></p>
<p><em>Above: ES optimization process, in a setting with only two 
parameters and a reward function (red = high, blue = low). At each 
iteration we show the current parameter value (in white), a population 
of jittered samples (in black), and the estimated gradient (white 
arrow). We keep moving the parameters to the top of the arrow until we 
converge to a local optimum. You can reproduce this figure with <a href="https://github.com/karpathy/randomfun/blob/master/es.ipynb">this notebook</a>.</em></p>
<p><strong>Code sample.</strong> To make the core algorithm concrete and
 to highlight its simplicity, here is a short example of optimizing a 
quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d">longer version</a> with more comments):</p>
<pre><code class="language-python"># simple example: minimize a quadratic around some solution point
import numpy as np
solution = np.array([0.5, 0.1, -0.3])
def f(w): return -np.sum((w - solution)**2)

npop = 50      # population size
sigma = 0.1    # noise standard deviation
alpha = 0.001  # learning rate
w = np.random.randn(3) # initial guess
for i in range(300):
  N = np.random.randn(npop, 3)
  R = np.zeros(npop)
  for j in range(npop):
    w_try = w + sigma*N[j]
    R[j] = f(w_try)
  A = (R - np.mean(R)) / np.std(R)
  w = w + alpha/(npop*sigma) * np.dot(N.T, A)
</code></pre>
<p><strong>Injecting noise in the parameters.</strong> Notice that the 
objective is identical to the one that RL optimizes: the expected 
reward. However, RL injects noise in the action space and uses 
backpropagation to compute the parameter updates, while ES injects noise
 directly in the parameter space. Another way to describe this is that 
RL is a “guess and check” on actions, while ES is a “guess and check” on
 parameters. Since we’re injecting noise in the parameters, it is 
possible to use deterministic policies (and we do, in our experiments). 
It is also possible to add noise in both actions and parameters to 
potentially combine the two approaches.</p>
<h4 id="tradeoffsbetweenesandrl">Tradeoffs between ES and RL</h4>
<p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p>
<ul>
<li><strong>No need for backpropagation</strong>. ES only requires the 
forward pass of the policy and does not require backpropagation (or 
value function estimation), which makes the code shorter and between 2-3
 times faster in practice. On memory-constrained systems, it is also not
 necessary to keep a record of the episodes for a later update. There is
 also no need to worry about exploding gradients in RNNs. Lastly, we can
 explore a much larger function class of policies, including networks 
that are not differentiable (such as in binary networks), or ones that 
include complex modules (e.g. pathfinding, or various optimization 
layers).</li>
<li><strong>Highly parallelizable.</strong> ES only requires workers to 
communicate a few scalars between each other, while in RL it is 
necessary to synchronize entire parameter vectors (which can be millions
 of numbers). Intuitively, this is because we control the random seeds 
on each worker, so each worker can locally reconstruct the perturbations
 of the other workers. Thus, all that we need to communicate between 
workers is the reward of each perturbation. As a result, we observed 
linear speedups in our experiments as we added on the order of thousands
 of CPU cores to the optimization.</li>
<li><strong>Higher robustness.</strong> Several hyperparameters that are
 difficult to set in RL implementations are side-stepped in ES. For 
example, RL is not “scale-free”, so one can achieve very different 
learning outcomes (including a complete failure) with different settings
 of the frame-skip hyperparameter in Atari. As we show in our work, ES 
works about equally well with any frame-skip.</li>
<li><strong>Structured exploration.</strong> Some RL algorithms 
(especially policy gradients) initialize with random policies, which 
often manifests as random jitter on spot for a long time. This effect is
 mitigated in Q-Learning due to epsilon-greedy policies, where the max 
operation can cause the agents to perform some consistent action for a 
while (e.g. holding down a left arrow). This is more likely to do 
something in a game than if the agent jitters on spot, as is the case 
with policy gradients. Similar to Q-learning, ES does not suffer from 
these problems because we can use deterministic policies and achieve 
consistent exploration.</li>
<li><strong>Credit assignment over long time scales.</strong> By 
studying both ES and RL gradient estimators mathematically we can see 
that ES is an attractive choice especially when the number of time steps
 in an episode is long, where actions have longlasting effects, or if no
 good value function estimates are available.</li>
</ul>
<p>Conversely, we also found some challenges to applying ES in practice.
 One core problem is that in order for ES to work, adding noise in 
parameters must lead to different outcomes to obtain some gradient 
signal. As we elaborate on in our paper, we found that the use of 
virtual batchnorm can help alleviate this problem, but further work on 
effectively parameterizing neural networks to have variable behaviors as
 a function of noise is necessary. As an example of a related 
difficulty, we found that in Montezuma’s Revenge, one is very unlikely 
to get the key in the first level with a random network, while this is 
occasionally possible with random actions.</p>
<h4 id="esiscompetitivewithrl">ES is competitive with RL</h4>
<p>We compared the performance of ES and RL on two standard RL 
benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo 
task (see examples below) contains a physically-simulated articulated 
figure, where the policy receives the positions of all joints and has to
 output the torques to apply at each joint in order to move forward. 
Below are some example agents trained on three MuJoCo control tasks, 
where the objective is to move forward:</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/out.gif" alt=""></p>
<p>We usually compare the performance of algorithms by looking at their 
efficiency of learning from data; as a function of how many states we’ve
 seen, what is our average reward? Here are the example learning curves 
that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477">TRPO</a> algorithm in this case):</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/es_vs_trpo_full.png" alt=""></p>
<p><strong>Data efficiency comparison</strong>. The comparisons above 
show that ES (orange) can reach a comparable performance to TRPO (blue),
 although it doesn’t quite match or surpass it in all cases. Moreover, 
by scanning horizontally we can see that ES is less efficient, but no 
worse than about a factor of 10 (note the x-axis is in log scale).</p>
<p><strong>Wall clock comparison</strong>. Instead of looking at the raw
 number of states seen, one can argue that the most important metric to 
look at is the wall clock time: how long (in number of seconds) does it 
take to solve a given problem? This quantity ultimately dictates the 
achievable speed of iteration for a researcher. Since ES requires 
negligible communication between workers, we were able to solve one of 
the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 
machines in only 10 minutes. As a comparison, in a typical setting 32 
A3C workers on one machine would solve this task in about 10 hours. It 
is also possible that the performance of RL could also improve with more
 algorithmic and engineering effort, but we found that naively scaling 
A3C in a standard cloud CPU setting is challenging due to high 
communication bandwidth requirements.</p>
<p>Below are a few videos of 3D humanoid walkers trained with ES. As we 
can see, the results have quite a bit of variety, based on which local 
minimum the optimization ends up converging into.</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/out-1.gif" alt=""></p>
<p>On Atari, ES trained on 720 cores in 1 hour achieves comparable 
performance to A3C trained on 32 cores in 1 day. Below are some result 
snippets on Pong, Seaquest and Beamrider. These videos show the 
preprocessed frames, which is exactly what the agent sees when it is 
playing:</p>
<p><img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/atari.gif" alt=""></p>
<p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p>
<h4 id="relatedwork">Related Work</h4>
<p>ES is an algorithm from the neuroevolution literature, which has a 
long history in AI and a complete literature review is beyond the scope 
of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf">Natural Evolution Strategies</a>
 by Wierstra et al. 2014. Compared to this work and much of the work it 
has inspired, our focus is specifically on scaling these algorithms to 
large-scale, distributed settings, finding components that make the 
algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p>
<p>It is also worth noting that neuroevolution-related approaches have 
seen some recent resurgence in the machine learning literature, for 
example with <a href="https://arxiv.org/abs/1609.09106">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580">“Convolution by Evolution”</a>.</p>
<h4 id="conclusion">Conclusion</h4>
<p>Our work suggests that neuroevolution approaches can be competitive 
with reinforcement learning methods on modern agent-environment 
benchmarks, while offering significant benefits related to code 
complexity and ease of scaling to large-scale distributed settings. We 
also expect that more exciting work can be done by revisiting other 
ideas from this line of work, such as indirect encoding methods, or 
evolving the network structure in addition to the parameters.</p>
<p><strong>Note on supervised learning</strong>. It is also important to
 note that supervised learning problems (e.g. image classification, 
speech recognition, or most other tasks in the industry), where one can 
compute the exact gradient of the loss function with backpropagation, 
are not directly impacted by these findings. For example, in our 
preliminary experiments we found that using ES to estimate the gradient 
on the MNIST digit recognition task can be as much as 1,000 times slower
 than using backpropagation. It is only in RL settings, where one has to
 estimate the gradient of the expected reward by sampling, where ES 
becomes competitive.</p>
<p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading  <a href="https://arxiv.org/abs/1703.03864">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter">Github repo</a>.</p>
</section></article></div>

    

    

    <script type="text/x-mathjax-config;executed=true">
      MathJax.Hub.Config({
        messageStyle: "none"
      });
    </script>
    <script src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/MathJax.js" id="">
    </script>










    
    <footer class="PostFooter">

  <div class="PostFooter-authors">
    <span class="PostFooter-authors--label">By</span>             <a href="https://blog.openai.com/tag/andrej-karpathy/" title="Andrej Karpathy" class="tag tag-59c2c23a1b5ef5001987046a andrej-karpathy">
          Andrej Karpathy</a>, 
        <a href="https://blog.openai.com/tag/tim-salimans/" title="Tim Salimans" class="tag tag-59c2c23a1b5ef50019870471 tim-salimans">
          Tim Salimans</a>, 
        <a href="https://blog.openai.com/tag/jonathan-ho/" title="Jonathan Ho" class="tag tag-59c2c23a1b5ef5001987046f jonathan-ho">
          Jonathan Ho</a>, 
        <a href="https://blog.openai.com/tag/peter-chen/" title="Peter Chen" class="tag tag-59c2c23a1b5ef5001987046c peter-chen">
          Peter Chen</a>, 
        <a href="https://blog.openai.com/tag/ilya-sutskever/" title="Ilya Sutskever" class="tag tag-59c2c2391b5ef5001987044f ilya-sutskever">
          Ilya Sutskever</a>, 
        <a href="https://blog.openai.com/tag/john-schulman/" title="John Schulman" class="tag tag-59c2c2391b5ef50019870450 john-schulman">
          John Schulman</a>, 
        <a href="https://blog.openai.com/tag/greg-brockman/" title="Greg Brockman" class="tag tag-59c2c2391b5ef5001987044a greg-brockman">
          Greg Brockman</a> &amp; 
        <a href="https://blog.openai.com/tag/szymon-sidor/" title="Szymon Sidor" class="tag tag-59c2c23a1b5ef5001987047a szymon-sidor">
          Szymon Sidor</a>.

  </div>

  <section class="PostFooter-social">
    <a class="SocialButton
              SocialButton--twitter" href="https://twitter.com/intent/tweet?text=Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning&amp;url=https://blog.openai.com/evolution-strategies/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
      Tweet
    </a>
    <a class="SocialButton
              SocialButton--facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://blog.openai.com/evolution-strategies/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
      Share
    </a>
    <!--<a class="icon-google-plus" href="https://plus.google.com/share?url=https://blog.openai.com/evolution-strategies/"
        onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
      <span class="hidden">Google+</span>
    </a>-->
  </section>


</footer>  


<section class="MorePosts">

  <header class="Shared-SectionHeader">
    <h1>Keep reading</h1>
    <div class="Shared-SectionHeader-linkWrap">
      <a class="Shared-SectionHeader-Link" href="https://blog.openai.com/">More</a>
    </div>
  </header>

  <div class="PostList-container">
    <div class="PostList
                PostList--cardify">

              <article class="Shared-Card post tag-scott-gray tag-alec-radford tag-durk-kingma tag-hash-research-release-post tag-hash-research-release-video-cover tag-hash-research-release-no-10" id="block-sparse-gpu-kernels">
        <a class="Shared-Card-anchor" href="https://blog.openai.com/block-sparse-gpu-kernels/">

          <span class="Shared-Card-background">
            <span class="Shared-Card-background-inner">


              <svg class="icon" width="100%" height="100%" viewBox="0 0 330 418">
              </svg>

            </span>
          </span>
          
          <span class="Shared-Card-inner">
            <h2 class="Shared-Card-title">
                    <span class="format-research-titles">Block-Sparse <br> GPU Kernels</span>
            </h2>

              
              <span class="Shared-Card-coverContainer">
                <span class="Shared-Card-coverContainer-inner">
                  <span class="Shared-Card-coverContainer-meta">
                    <time datetime="2017-12-06">
                      <span class="Shared-card-coverContainer-meta-date">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        No. 10
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        

                      </span>
                      <span class="Shared-card-coverContainer-meta-date">Dec 6</span>
                      <span class="Shared-card-coverContainer-meta-date">2017</span>
                    </time>
                  </span>
                  
                  <div class="Shared-Card-coverContainer-cover" data-parallax-z-index="1">
                    <video autoplay="true" loop="true" poster="https://d4mucfpksywv.cloudfront.net/research-covers/block-sparse-gpu-kernels/2x-no-mark-animated-poster.jpg">
	<source src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/2x-no-mark-animated.mp4" media="screen and (-webkit-min-device-pixel-ratio: 1.5), screen and (min-resolution: 144dpi)">
	<source src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/2x-no-mark-animated.webm" media="screen and (-webkit-min-device-pixel-ratio: 1.5), screen and (min-resolution: 144dpi)">
	<source src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/1x-no-mark-animated.mp4">
    <source src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/1x-no-mark-animated.webm">

	<img src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/1x-no-mark.png" srcset="https://d4mucfpksywv.cloudfront.net/research-covers/block-sparse-gpu-kernels/1x-no-mark.jpg 1x,
            https://d4mucfpksywv.cloudfront.net/research-covers/block-sparse-gpu-kernels/2x-no-mark.jpg 2x" alt="Block-Sparse GPU Kernels">
</video>
                    <span class="Shared-Card-glare"></span>
                  </div>
                </span>
              </span>





          </span>
        </a>
     </article><article class="Shared-Card post tag-christopher-berner tag-hash-software tag-hash-design-card-blue" id="scaling-kubernetes-to-2500-nodes">
        <a class="Shared-Card-anchor" href="https://blog.openai.com/scaling-kubernetes-to-2500-nodes/">

          <span class="Shared-Card-background">
              <span class="Shared-Card-glare"></span>
            <span class="Shared-Card-background-inner">


              <svg class="icon" width="100%" height="100%" viewBox="0 0 330 418">
                    <use class="background-shape" xlink:href="#square" x="0" y="0"></use>
              </svg>

            </span>
          </span>
          
          <span class="Shared-Card-inner">
            <h2 class="Shared-Card-title">
                Scaling Kubernetes to 2,500 Nodes
            </h2>




                <span class="Shared-Card-iconContainer">
                  <span class="Shared-Card-iconCSS">
                    <span class="Shared-Card-iconCSS-img
                                
                                Shared-Card-iconCSS-img--software
                                
                                
                                "></span>
                  </span>
                </span>

              

              <span class="Shared-Card-meta">
                <span class="Shared-Card-meta-category"></span>
                <span class="Shared-Card-meta-date">
                  <time datetime="2018-01-18">Jan 18, 2018</time>
                </span>
              </span>
            
            


          </span>
        </a>
     </article>                            <article class="Shared-Card post tag-hash-research tag-kevin-frans tag-jonathan-ho tag-peter-chen tag-pieter-abbeel tag-john-schulman tag-hash-design-card-black" id="learning-a-hierarchy">
        <a class="Shared-Card-anchor" href="https://blog.openai.com/learning-a-hierarchy/">

          <span class="Shared-Card-background">
              <span class="Shared-Card-glare"></span>
            <span class="Shared-Card-background-inner">


              <svg class="icon" width="100%" height="100%" viewBox="0 0 330 418">
                    <use class="background-shape" xlink:href="#hexagon" x="0" y="0"></use>
              </svg>

            </span>
          </span>
          
          <span class="Shared-Card-inner">
            <h2 class="Shared-Card-title">
                Learning a Hierarchy
            </h2>




                <span class="Shared-Card-iconContainer">
                  <span class="Shared-Card-iconCSS">
                    <span class="Shared-Card-iconCSS-img
                                
                                
                                Shared-Card-iconCSS-img--research
                                
                                "></span>
                  </span>
                </span>

              

              <span class="Shared-Card-meta">
                <span class="Shared-Card-meta-category"></span>
                <span class="Shared-Card-meta-date">
                  <time datetime="2017-10-26">Oct 26, 2017</time>
                </span>
              </span>
            
            


          </span>
        </a>
     </article>
    </div>
  </div>
</section>

    

    <footer class="Shared-Footer">

  <div class="Shared-Footer-linksContainer">

    <div class="container-fluid">
<!--      
        <div class="row">
          <div class="col-xs-9
                      col-sm-6">-->

        <div class="Shared-Footer-wrap">

            <ul class="Shared-Footer-list">

              <li class="">
                <a class="Shared-Footer-link
                          Shared-Footer-link--listLink" href="https://openai.com/research/">Research</a>
              </li>

              <li class="">
                <a class="Shared-Footer-link
                          Shared-Footer-link--listLink" href="https://openai.com/systems/">Systems</a>
              </li>

              <li class="Shared-Footer-separated-x">
                <a class="Shared-Footer-link
                          Shared-Footer-link--listLink" href="https://openai.com/about/">About</a>
              </li>
              <li class="">
                <a class="Shared-Footer-link
                          Shared-Footer-link--listLink" href="https://blog.openai.com/">Blog</a>
              </li>
              <li class="">
                <a class="Shared-Footer-link
                          Shared-Footer-link--listLink" href="https://openai.com/jobs/">Jobs</a>
              </li>
            </ul>

          <!--</div>

          <div class="col-xs-3
                      col-sm-6">-->
            <section class="Shared-Footer-social">  
              <a class="Shared-Footer-link
                        Shared-Footer-link--image
                        Shared-Footer-link--twitter" href="https://twitter.com/openai">Twitter</a>
              <a class="Shared-Footer-link
                        Shared-Footer-link--image
                        Shared-Footer-link--facebook" href="https://www.facebook.com/openai.research">Facebook</a>
            </section>
          <!--</div>

        </div>-->

        </div>

    </div>
      
  </div><!-- ./Shared-Footer-linksContainer -->
</footer><!-- ./Shared-Footer -->
  </main>

  
  <script type="text/javascript" src="Evolution%20Strategies%20as%20a%20Scalable%20Alternative%20to%20Reinforcement%20Learning_files/app.js"></script>



<svg width="0" height="0" viewBox="0 0 330 418" style="visibility:hidden;width:0;height:0;display:block;">
  
  <filter id="f1" x="-50%" y="-50%" width="200%" height="200%">
    <feGaussianBlur in="SourceGraphic" stdDeviation="30"></feGaussianBlur>
  </filter>

  <defs>
    <linearGradient id="blue">
      <stop offset="0" stop-color="rgb(94,33,217)"></stop>
      <stop offset="1" stop-color="rgb(18,165,226)"></stop>
    </linearGradient>
    <linearGradient id="blue_green">
      <stop offset="0" stop-color="rgb(47,134,215)"></stop>
      <stop offset="1" stop-color="rgb(28,245,186)"></stop>
    </linearGradient>
    <linearGradient id="cyan">
      <stop offset="0" stop-color="rgb(64,100,216)"></stop>
      <stop offset="1" stop-color="rgb(74,216,221)"></stop>
    </linearGradient>
    <linearGradient id="purple">
      <stop offset="0" stop-color="rgb(90,64,216)"></stop>
      <stop offset="1" stop-color="rgb(216,74,221)"></stop>
    </linearGradient>
    <linearGradient id="red">
      <stop offset="0" stop-color="rgb(183,24,102)"></stop>
      <stop offset="1" stop-color="rgb(235,52,52)"></stop>
    </linearGradient>
    <linearGradient id="orange">
      <stop offset="0" stop-color="rgb(211,56,56)"></stop>
      <stop offset="1" stop-color="rgb(242,153,38)"></stop>
    </linearGradient>
    <linearGradient id="yellow">
      <stop offset="0" stop-color="rgb(232,91,6)"></stop>
      <stop offset="1" stop-color="rgb(229,231,42)"></stop>
    </linearGradient>
    <linearGradient id="green">
      <stop offset="0" stop-color="rgb(13,163,130)"></stop>
      <stop offset="1" stop-color="rgb(103,228,78)"></stop>
    </linearGradient>
    <linearGradient id="black">
      <stop offset="0" stop-color="rgb(0,0,0)"></stop>
      <stop offset="1" stop-color="rgb(38,38,38)"></stop>
    </linearGradient>
    <linearGradient id="slate-dark">
      <stop offset="0" stop-color="rgb(34,34,34)"></stop>
      <stop offset="1" stop-color="rgb(69,69,77)"></stop>
    </linearGradient>
    <linearGradient id="slate-mid">
      <stop offset="0" stop-color="rgb(59,62,76)"></stop>
      <stop offset="1" stop-color="rgb(102,102,120)"></stop>
    </linearGradient>
    <linearGradient id="slate-light">
      <stop offset="0" stop-color="rgb(137,137,155)"></stop>
      <stop offset="1" stop-color="rgb(181,181,196)"></stop>
    </linearGradient>


    <!-- Outer shape -->
    <symbol id="outer">
      <path d="M268.7 12c-10.16-4.21-26.79-4.21-37 0l-137 56.74C84.61 73 72.85 84.71 68.64 94.87l-56.74 137c-4.21 10.16-4.21 26.79 0 37l56.74 137c4.21 10.16 16 21.92 26.13 26.13l137 56.74c10.16 4.21 26.79 4.21 37 0l137-56.74c10.16-4.21 21.92-16 26.13-26.13l56.74-137c4.21-10.16 4.21-26.79 0-37l-56.74-137c-4.21-10.16-16-21.92-26.13-26.13z"></path>
    </symbol>

    <!-- Inner shape -->
    <symbol id="inner">
      <path d="M250.22 56.28l-137.2 56.84-56.84 137.21 56.84 137.21 137.21 56.83 137.2-56.84 56.84-137.21-56.84-137.2-137.21-56.84z"></path>
    </symbol>

    <mask id="outerMask" maskUnits="userSpaceOnUse" x="0" y="0" width="500" height="500">
      <rect x="0" y="0" width="500" height="500" fill="white"></rect>
      <use filter="url(#f1)" xlink:href="#outer"></use>
    </mask>

    <mask id="outlineMask" maskUnits="userSpaceOnUse" x="0" y="0" width="500" height="500">
      <use fill="white" xlink:href="#outer"></use>
      <use fill="black" xlink:href="#inner"></use>
    </mask>

  </defs>
  <symbol id="circle">
    <path class="cls-1" d="M46.633,169.834A171,171,0,1,1-162.8,290.749,171,171,0,0,1,46.633,169.834Z"></path>
    <circle class="cls-2" cx="250.625" cy="98.984" r="171.063"></circle>
  </symbol>
  <symbol id="hexagon">
    <path d="M103.728-104.851a20.335,20.335,0,0,0-13.505,0L-16.926-60.458a20.338,20.338,0,0,0-9.549,9.55L-70.851,56.249a20.338,20.338,0,0,0,0,13.506L-26.458,176.9a20.343,20.343,0,0,0,9.551,9.55L90.249,230.828a20.336,20.336,0,0,0,13.505,0L210.9,186.435a20.337,20.337,0,0,0,9.549-9.55L264.828,69.729a20.336,20.336,0,0,0,0-13.505L220.435-50.926a20.337,20.337,0,0,0-9.551-9.549Z"></path>
    <path d="M186.713,201.118a20.332,20.332,0,0,0-13.506,0l-107.16,44.4a20.342,20.342,0,0,0-9.55,9.551L12.117,362.232a20.341,20.341,0,0,0,0,13.507L56.515,482.9a20.335,20.335,0,0,0,9.551,9.55l107.166,44.38a20.334,20.334,0,0,0,13.507,0l107.159-44.4a20.339,20.339,0,0,0,9.55-9.552l44.38-107.166a20.338,20.338,0,0,0,0-13.506l-44.4-107.16a20.343,20.343,0,0,0-9.552-9.55Z"></path>
  </symbol>
  <symbol id="square">
    <path d="M184.988,276.989a14.03,14.03,0,0,1,0,19.774L38.917,442.917a14.011,14.011,0,0,1-19.762,0L-126.916,296.762a14.027,14.027,0,0,1,0-19.773L19.155,130.835a14.009,14.009,0,0,1,19.762,0Z" transform="translate(0 -47)"></path>
    <path d="M464.677,206.029a14.843,14.843,0,0,1,0,20.932L309.948,381.673a14.846,14.846,0,0,1-20.933,0L134.286,226.96a14.844,14.844,0,0,1,0-20.931L289.015,51.317a14.847,14.847,0,0,1,20.934,0Z" transform="translate(0 -47)"></path>
  </symbol>
  <symbol id="triangle">
    <path d="M71.037-26.913c-4.7-8.144-12.378-8.144-17.074,0L-130,292.192c-4.695,8.144-.854,14.808,8.537,14.808H246.462c9.391,0,13.232-6.664,8.537-14.808Z"></path>
    <path d="M305-.476C299.5-10,290.5-10,285-.476L69.5,372.683C64,382.208,68.5,390,79.5,390h431c11,0,15.5-7.792,10-17.317Z"></path>
  </symbol>


  <symbol id="hexagon-icon">
    <g mask="url(#outlineMask)">
      <rect mask="url(#outerMask)" x="-50%" y="-50%" width="200%" height="200%" fill="black"></rect>
      <rect mask="url(#outerMask)" x="-50%" y="-50%" width="200%" height="200%" fill="black"></rect>
      <rect mask="url(#outerMask)" x="-50%" y="-50%" width="200%" height="200%" fill="black"></rect>
    </g>
  </symbol>
</svg>



  <!-- Segment snippet -->
  <script type="text/javascript">
    !function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on"];analytics.factory=function(t){return function(){var e=Array.prototype.slice.call(arguments);e.unshift(t);analytics.push(e);return analytics}};for(var t=0;t<analytics.methods.length;t++){var e=analytics.methods[t];analytics[e]=analytics.factory(e)}analytics.load=function(t){var e=document.createElement("script");e.type="text/javascript";e.async=!0;e.src=("https:"===document.location.protocol?"https://":"http://")+"cdn.segment.com/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)};analytics.SNIPPET_VERSION="4.0.0";
    analytics.load("6gG9RqmGss3RlZ1wdayfXrkImRHAx0hE");
    analytics.page("Blog", "Evolution Strategies as a Scalable Alternative to Reinforcement Learning");
    }}();
  </script>



</body></html>